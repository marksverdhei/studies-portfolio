{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbqmC_QDWXga"
   },
   "source": [
    "# IN4080, 2020, Mandatory assignment 2, part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markus Heiervang - markuhei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1idlhSEWXgd"
   },
   "source": [
    "### About the assignment\n",
    "**Your answer should be delivered in devilry no later than Friday, 9 October at 23:59**\n",
    "\n",
    "Mandatory assignment 2 consists of two parts \n",
    "\n",
    "* Part A on tagging and sequence classification (=this file)\n",
    "* Part B on word embeddings (separate document)\n",
    "    \n",
    "You should answer both parts. It is possible to get 65 points part A, 35 points on part B, 100 points altogether. You are required to get at least 60 points to pass. It is more important that you try to answer each question than that you get it correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUJouTYCWXgf"
   },
   "source": [
    "### General requirements:\n",
    "\n",
    "\n",
    "- We assume that you have read and are familiar with IFI's requirements and guidelines for mandatory assignments\n",
    "    - https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html\n",
    "    - https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html\n",
    "- This is an individual assignment. You should not deliver joint submissions.\n",
    "- You may redeliver in Devilry before the deadline, but include all files in the last delivery. Only the last delivery will be read!\n",
    "- If you deliver more than one file, put them into a zip-archive.\n",
    "- Name your submission your_username_in4080_mandatory_2\n",
    "\n",
    "The delivery can take one of two forms:\n",
    "\n",
    "- Alternative A:\n",
    "\n",
    "    - Deliver the code files. \n",
    "    - In addition, deliver a separate pdf file containing results from the runs together with answers to the text questions.\n",
    "    \n",
    "- Alternative B:\n",
    "\n",
    "    - A jupyter notebook containing code, answers to the text questions in markup and optionally results from the runs.\n",
    "    - In addition, a pdf-version of the notebook where (in addition) all the results of the runs are included.\n",
    "\n",
    "Whether you use the first or second alternative, make sure that the code runs at the IFI machines after\n",
    "\n",
    "-\texport PATH=/opt/ifi/anaconda3/bin/:$PATH\n",
    "\n",
    "or on your own machine in the environment we provided, see \n",
    "- https://www.uio.no/studier/emner/matnat/ifi/IN4080/h20/lab-setup/.\n",
    "\n",
    "If you use any additional packages, they should be included in your delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "uVONk-acWXgh"
   },
   "source": [
    "### Goals of part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djP7mpAzWXgi"
   },
   "source": [
    "In this part we will experiment with sequence classification and tagging. We will combine some of the tools for tagging from NLTK with scikit-learn to build various taggers.We will start with simple examples from NLTK where the tagger only considers the token to be tagged---not its context---and work towards more advanced logistic regression taggers (also called maximum entropy taggers). Finally,  we will compare to some tagging algorithms installed in NLTK.\n",
    "\n",
    "In this set you will get more experience with\n",
    "\n",
    "- baseline for a tagger and more generally for classification\n",
    "- how different tag sets may result in different accuracies\n",
    "- feature selection\n",
    "- the effect of the machine learner\n",
    "- smoothing \n",
    "- evaluation\n",
    "- in-domain and out-of-domain evaluation\n",
    "\n",
    "To get a good tagger, you need a reasonably sized tagged training corpus. Ideally, we would have used the complete Brown corpus in this exercise, but it turns out that some of the experiments we will run, will be time consuming. Hence, we will follow the NLTK book and use only the News section. Since this is a rather homogeneous domain, and we also pick our test data from the same domain, we can still get decent results.\n",
    "\n",
    "Towards the end of the exercise set, we will see what happens if we take our best settings from the News section to a bigger domain.\n",
    "\n",
    "Beware that even with this reduced corpus, some of the experiments will take several minutes. And when we build the full tagger in exercise 5, an experiment may take half an hour. So make sure you start the work early enough. (You might do other things while the experiments are running.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kXGYKbwWXgk"
   },
   "source": [
    "### Replicating NLTK Ch. 6 \n",
    "\n",
    "We jump into the NLTK book, chapter 6, the sections 6.1.5 Exploiting context and 6.1.6 Sequence classification. You are advised to read them before you start.\n",
    "\n",
    "We start by importing NLTK and the tagged sentences from the news-section from Brown, similarly to the NLTK book.\n",
    "\n",
    "Then we split the set of sentences into a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGk0yQnKWXgl"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import time\n",
    "import random\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MyErc4rWXgr"
   },
   "source": [
    "Like NLTK, our tagger will have two parts, a feature extractor, here called **pos_features**, and a general class for building taggers, **ConsecutivePosTagger**.\n",
    "\n",
    "We have made a few adjustments to the NLTK setup. We are using the *pos_features* from section 6.1.5 together with the *ConsecutivePosTagger* from section 6.1.6. The *pos_features* in section 1.5 does not consider history, but to get a format that works together with *ConsecutivePosTagger*, we have included an argument for history in *pos_features*, which is not used initially. (It get used by the *pos_features* in section 6.1.6 of the NLTK book, and you may choos to use it later in this set).\n",
    "\n",
    "Secondly, we have made the *feature_extractor* a parameter to *ConsecutivePosTagger*, so that it can easily be replaced by other feature extractors while keeping *ConsecutivePosTagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ_O-JL9WXgs"
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo7vpaPvWXgx"
   },
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, features=pos_features):\n",
    "        self.features = features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSce-lPnWXg3"
   },
   "source": [
    "Following the NLTK bok, we train and test a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WHXW8_rzWXg4",
    "outputId": "c1534688-55c4-46bf-f658-f0e188ccc848"
   },
   "outputs": [],
   "source": [
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(round(tagger.evaluate(test_sents), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y2ekc7JWXg_"
   },
   "source": [
    "This should give results comparable to the NLTK book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8XBVn4KWXhA"
   },
   "source": [
    "## Ex 1: Tag set and baseline (10 points)\n",
    "### Part a. Tag set and experimental set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFCdvAnSWXhB"
   },
   "source": [
    "We will simplify and use the universal pos tagset in this exercise. One main reason is that it makes the experiments run faster.\n",
    "\n",
    "We will be a little more cautious than the NLTK-book, when it comes to training and test sets. We will split the News-section into three sets\n",
    "\n",
    "- 10% for final testing which we tuck aside for now, call it *news_test*\n",
    "- 10% for development testing, call it *news_dev_test*\n",
    "- 80% for training, call it *news_train*\n",
    "\n",
    "- Make the data sets, and repeat the training and evaluation with *news_train* and *news_dev_test*.\n",
    "- Please use 4 counting decimal places and stick to that throughout the exercise set.\n",
    "\n",
    "How is the result compared to using the full brown tagset? Why do you think one of the tagsets yields higher scores than the other one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-IPdzVvWXhC"
   },
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "def train_dev_test_split(original_data, ratios=(0.8, 0.1), seed=42):\n",
    "    assert sum(ratios)+ratios[1] == 1.0\n",
    "    random.seed(SEED)\n",
    "    data = list(original_data)\n",
    "    random.shuffle(data)\n",
    "    N = len(data)\n",
    "    a, b = int(N*ratios[0]), int(N*ratios[1])\n",
    "    train = data[:a]\n",
    "    dev = data[a:-b]\n",
    "    test = data[-b:]\n",
    "    return train, dev, test\n",
    "\n",
    "news_train, news_dev, news_test = train_dev_test_split(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yIHM3cUAWXhH",
    "outputId": "503f005c-3aff-4efa-d8eb-9d599bc50002"
   },
   "outputs": [],
   "source": [
    "tagger = ConsecutivePosTagger(news_train)\n",
    "print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBIk8jBsWXhM"
   },
   "source": [
    "The results were better. The universal tagset contains fewer tags, meaning that the classification becomes more narrow. In turn, this means we have more datasamples per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfjN2eXsWXhN"
   },
   "source": [
    "### Part b. Baseline \n",
    "\n",
    "One of the first things we should do in an experiment like this, is to establish a reasonable baseline. A reasonable baseline here is the Most Frequent Class baseline.\n",
    "Each word which is seen during training should get its most frequent tag from the training. \n",
    "For words not seen during training, we simply use the most frequent overall tag.\n",
    "\n",
    "With news_train as training set and news_dev_set as valuation set, what is the accuracy of this baseline?\n",
    "\n",
    "Does the tagger from part (a) using the features from the NLTK book beat the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DJoZoMRdWXhO",
    "outputId": "ab959500-aa31-457d-d3bd-dca6972b261a"
   },
   "outputs": [],
   "source": [
    "class BaselineTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_set):\n",
    "        cfd = nltk.ConditionalFreqDist((w.lower(), t) for doc in news_train for w, t in doc)\n",
    "        self.mc_labels = {term: dist.max() for term, dist in cfd.items()}\n",
    "        self.most_common_label = nltk.FreqDist(self.mc_labels).most_common()\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        return ((term, self.mc_labels.get(term.lower(), self.most_common_label)) for term in sentence)\n",
    "\n",
    "blt = BaselineTagger(news_train)\n",
    "round(blt.evaluate(news_dev), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Msym0KvWXhS"
   },
   "source": [
    "No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-J2N-JaWXhT"
   },
   "source": [
    "## Ex2: scikit-learn and tuning (10 points)\n",
    "Our goal will be to improve the tagger compared to the simple suffix-based tagger. For the further experiments, we move to scikit-learn which yields more options for considering various alternatives. We have reimplemented the ConsecutivePosTagger to use scikit-learn classifiers below. We have made the classifier a parameter so that it can easily be exchanged. We start with the BernoulliNB-classifier which should correspond to the way it is done in NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEBUzX-_WXhU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents, \n",
    "                 features=pos_features, clf = BernoulliNB()):\n",
    "        # Using pos_features as default.\n",
    "        self.features = features\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_features.append(featureset)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        v = DictVectorizer()\n",
    "        X_train = v.fit_transform(train_features)\n",
    "        y_train = np.array(train_labels)\n",
    "        clf.fit(X_train, y_train)\n",
    "        self.classifier = clf\n",
    "        self.dict = v\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        test_features = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            test_features.append(featureset)\n",
    "        X_test = self.dict.transform(test_features)\n",
    "        tags = self.classifier.predict(X_test)\n",
    "        return zip(sentence, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diwdRT2cWXhY"
   },
   "source": [
    "### Part a.\n",
    "Train the ScikitConsecutivePosTagger on the *news_train* set and test on the *news_dev_test* set with the *pos_features*. Do you get the same result as with the same data and features and the NLTK code in exercise 1a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBsiAC40WXhZ",
    "outputId": "d8d45913-3347-4e20-8067-0c0cacdef65c"
   },
   "outputs": [],
   "source": [
    "tagger = ScikitConsecutivePosTagger(news_train)\n",
    "round(tagger.evaluate(news_dev), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-U4wWRWWXhe"
   },
   "source": [
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSUW6tUGWXhf"
   },
   "source": [
    "### Part b.\n",
    "I get inferior results compared to using the NLTK set-up with the same feature extractors. The only explanation I could find is that the smoothing is too strong. BernoulliNB() from scikit-learn uses Laplace smoothing as default (\"add-one\"). The smoothing is generalized to Lidstone smoothing which is expressed by the alpha parameter to BernoulliNB(alpha=...)\n",
    "Therefore, try again with alpha in [1, 0.5, 0.1, 0.01, 0.001, 0.0001]. What do you find to be the best value for alpha?\n",
    "\n",
    "With the best choice of alpha, do you get the same results as with the NLTK code in exercise 1a, worse results or better results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Iv8ekpzUWXhg",
    "outputId": "79425ab4-b329-4217-f602-44e1c49bce57"
   },
   "outputs": [],
   "source": [
    "alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for a in alphas:\n",
    "    tagger = ScikitConsecutivePosTagger(news_train, clf=BernoulliNB(alpha=a))\n",
    "    print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_D2ixGlWXhm"
   },
   "source": [
    "Most of the results are better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llqj9RGsWXho"
   },
   "source": [
    "### Part c.\n",
    "To improve the results, we may change the feature selector or the machine learner. We start with a simple improvement of the feature selector. The NLTK selector considers the previous word, but not the word itself. Intuitively, the word itself should be a stronger feature. Extend the NLTK feature selector with a feature for the token to be tagged. Rerun the experiment with various alphas and record the results. Which alpha gives the best accuracy and what is the accuracy?\n",
    "\n",
    "Did the extended feature selector beat the baseline? Intuitively, it should get as least as good accuracy as the baseline. Explain why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Ylk17XXaWXhp",
    "outputId": "13dae06e-7877-490d-c59b-348cee03668b"
   },
   "outputs": [],
   "source": [
    "def better_pos_features(sentence, i, history): \n",
    "    features = { \n",
    "                 \"suffix(0)\": sentence[i],\n",
    "                 \"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "alphas = [1, 0.5, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for a in alphas:\n",
    "    tagger = ScikitConsecutivePosTagger(news_train, features=better_pos_features, clf=BernoulliNB(alpha=a))\n",
    "    print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMp5p4w-WXht"
   },
   "source": [
    "The performance is much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "335Bige-WXhu"
   },
   "source": [
    "## Ex 3: Logistic regression (10 points)\n",
    "### Part a.\n",
    "We proceed with the best feature selector from the last exercise. We will study the effect of the learner. Import *LogisticRegression* and use it with standard settings instead of *BernoulliNB*. Train on *news_train* and test on *news_dev_test* and record the result. Is it better than the best result with Naive Bayes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-oRgwgkWXhv",
    "outputId": "b6cceeb7-de45-41d2-8e8b-e8ccab1dad20"
   },
   "outputs": [],
   "source": [
    "tagger = ScikitConsecutivePosTagger(news_train, clf=LogisticRegression(solver=\"lbfgs\", max_iter=500))\n",
    "print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBWadr_sWXh0"
   },
   "source": [
    "### Part b.\n",
    "Similarly to the Naive Bayes classifier, we will study the effect of smoothing. Smoothing for LogisticRegression is done by regularization. In scikit-learn, regularization is expressed by the parameter C. A smaller C means a heavier smoothing. (C is the inverse of the parameter $\\alpha$ in the lectures.) Try with C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value which yields the best result.\n",
    "\n",
    "Which C gives the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "id": "SuwlttXnWXh1",
    "outputId": "f9813a17-bfbe-41c2-ce43-9e757988eebc"
   },
   "outputs": [],
   "source": [
    "cs = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] \n",
    "\n",
    "for c in cs:\n",
    "    tagger = ScikitConsecutivePosTagger(news_train, features=better_pos_features, clf=LogisticRegression(C=c))\n",
    "    print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTIRgpk8WXh6"
   },
   "source": [
    "## Ex 4: Features (10 points)\n",
    "### Part a.\n",
    "We will now stick to the LogisticRegression() with the optimal C from the last point and see whether we are able to improve the results further by extending the feature extractor with more features. First, try adding a feature for the next word in the sentence, and then train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YDLxLu2WXh6",
    "outputId": "91a4eafa-219e-4107-a802-64dcda809dad"
   },
   "outputs": [],
   "source": [
    "best_param = 10\n",
    "tagger = ScikitConsecutivePosTagger(\n",
    "    news_train, \n",
    "    features=lambda sentence, i, history: { \n",
    "        \"word\": sentence[i],\n",
    "        \"prev-word\": \"<START>\" if not i else sentence[i-1],\n",
    "        \"next-word\": \"<END>\" if i == len(sentence)-1 else sentence[i+1],\n",
    "        \"suffix(1)\": sentence[i][-1:],\n",
    "        \"suffix(2)\": sentence[i][-2:],\n",
    "        \"suffix(3)\": sentence[i][-3:]\n",
    "    }, \n",
    "clf=LogisticRegression(C=best_param))\n",
    "print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc3yYbB9WXh_"
   },
   "source": [
    "### Part b.\n",
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may consider. Some candidates: is the word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? Consider larger contexts? etc. What is the best feature set you can come up with? Train and test various feature sets and select the best one. \n",
    "\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make references to the sources and explain what you got from them.\n",
    "\n",
    "Observe that the way *ScikitConsecutivePosTagger.tag()* is written, it extracts the features from a whole sentence before it tags it. Hence it does not support  preceding tags as features. It is possible to rewrite *ScikitConsecutivePosTagger.tag()* to extract features after reading each word, and to use the *history* which keeps the preceding tags in the sentence. If you like, you may try it. However, we got surprisingly little gain from including preceding tags as features, and you are not requested to trying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa_v75SxWXh_",
    "outputId": "11ecab16-9fb3-4978-c06f-a27bba2f3543",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = lambda sentence, i, history: { \n",
    "    \"word\": sentence[i],\n",
    "    \"prefix(1)\": sentence[i][:1],\n",
    "    \"prefix(2)\": sentence[i][:2],\n",
    "    \"prefix(3)\": sentence[i][:3],\n",
    "    \"prev-prev-word\": \"<START-START>\" if i<1 else sentence[i-2],\n",
    "    \"prev-word\": \"<START>\" if not i else sentence[i-1],\n",
    "    \"next-word\": \"<END>\" if i == len(sentence)-1 else sentence[i+1],\n",
    "    \"next-next-word\": \"<END-END>\" if i>=len(sentence)-2 else sentence[i+2],\n",
    "    \"suffix(1)\": sentence[i][-1:],\n",
    "    \"suffix(2)\": sentence[i][-2:],\n",
    "    \"suffix(3)\": sentence[i][-3:],\n",
    "}\n",
    "\n",
    "tagger = ScikitConsecutivePosTagger(\n",
    "    news_train,\n",
    "    features=features,\n",
    "    clf=LogisticRegression(C=best_param)\n",
    ")\n",
    "\n",
    "print(round(tagger.evaluate(news_dev), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3cgRXCaWXiF"
   },
   "source": [
    "## Ex5: Larger corpus and evaluation (15 points)\n",
    "### Part a.\n",
    "We can now test our best tagger so far on the *news_test* set. \n",
    "Do that. How is the result compared to testing on *news_dev_test*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnHbmplFWXiF",
    "outputId": "83e070fd-bd6c-4f99-aca1-4008dc4d4ea9"
   },
   "outputs": [],
   "source": [
    "round(tagger.evaluate(news_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLuBCWkCWXiJ"
   },
   "source": [
    "### Part b.\n",
    "But we are looking for bigger fish. How good is our settings when trained on a bigger corpus?\n",
    "\n",
    "We will use nearly the whole Brown corpus. But we will take away two categories for later evaluation: *adventure* and *hobbies*. We will also initially stay clear of *news* to be sure not to mix training and test data.\n",
    "\n",
    "Call the Brown corpus with all categories except these three for *rest*. Shuffle the tagged sentences from *rest* and remeber to use the universal pos tagset. Then split the set into 80%-10%-10%: *rest_train*, *rest_dev_test*, *rest_test*.\n",
    "\n",
    "We can then merge these three sets with the corresponding sets from *news* to get final training and test sets:\n",
    "\n",
    "* `train = rest_train + news_train`\n",
    "* `test = rest_test + news_test`\n",
    "\n",
    "The first we should do is to establish a new baseline. Do this similarly to the way you did for the news corpus above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIRBhxkXWXiK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = set(brown.categories()) - {'adventure', 'hobbies'}\n",
    "brown_sents = brown.tagged_sents(categories=categories, tagset='universal')\n",
    "brown_train, brown_dev, brown_test = train_dev_test_split(brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjEqz3uTWXiO",
    "outputId": "401a70de-e759-4988-9294-35792d1fd2f4"
   },
   "outputs": [],
   "source": [
    "len(brown_train), len(news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlPQYcXIWXiU",
    "outputId": "2f9ec936-4e44-43ac-c89a-d8de30ad8463"
   },
   "outputs": [],
   "source": [
    "baseline = BaselineTagger(brown_train)\n",
    "round(baseline.evaluate(brown_dev), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o__G1mOyWXiY"
   },
   "source": [
    "### Part c.\n",
    "We can then build our tagger for this larger domain. Use the best settings from the earlier exercises, train on *train* and test on *test*. What is the accuracy of your tagger? \n",
    "\n",
    "#### Warning: Running this experiment may take 15-30 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPI4oblWWXiZ"
   },
   "outputs": [],
   "source": [
    "def benchmark_tagger(train_func, train_set, test_set):\n",
    "    \"Trains and tests a tagger\"\n",
    "    train_dur = time.time()\n",
    "    tagger = train_func(train_set)\n",
    "    train_dur = time.time() - train_dur\n",
    "    \n",
    "    test_dur = time.time()\n",
    "    accuracy = tagger.evaluate(test_set)\n",
    "    test_dur = time.time() - test_dur\n",
    "    \n",
    "    return {\n",
    "        \"tagger\": tagger,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_dur\": train_dur,\n",
    "        \"test_dur\": test_dur\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCk6JHBUWXig",
    "outputId": "d8b25c32-484c-45f5-a263-0e8d1632be13"
   },
   "outputs": [],
   "source": [
    "logreg_tagger = lambda train: ScikitConsecutivePosTagger(\n",
    "    train,\n",
    "    features=features,\n",
    "    clf=LogisticRegression(C=best_param)\n",
    ")\n",
    "\n",
    "results = benchmark_tagger(logreg_tagger, brown_train, brown_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um3eyoqnWXil"
   },
   "source": [
    "### Part d.\n",
    "Test the big tagger first on *adventures* then on *hobbies*. Discuss in a few sentences why you see different results from when testing on *test*. Why do you think you got different results on *adventures* from *hobbies*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_orVKSV4WXil",
    "outputId": "39320828-dec8-4cb7-cf0e-a34d0baf5cdf"
   },
   "outputs": [],
   "source": [
    "big_tagger = results[\"tagger\"]\n",
    "\n",
    "for c in \"hobbies\", \"adventure\":\n",
    "    print(f\"testing on {c}\")\n",
    "    sents = brown.tagged_sents(categories=c, tagset=\"universal\")\n",
    "    print(\"Accuracy: \", big_tagger.evaluate(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFA77mJaWXip"
   },
   "source": [
    "Hobbies and adventures are similar categories, but fairly different from the other ones. They will contain some different terms from the rest of the corpus, and the test set. Categories will trivially determine the difference of the data to some extent, and the test data was part of the same categories as the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "2bIP7b3ZWXiq"
   },
   "source": [
    "## Ex6: Comparing to other taggers (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV8GxfbEWXir"
   },
   "source": [
    "### Part a.\n",
    "In the lectures, we spent quite some time on the HMM-tagger. NLTK comes with an HMM-tagger which we may train and test on our own corpus. It can be trained by \n",
    "\n",
    "`news_hmm_tagger = nltk.HiddenMarkovModelTagger.train(news_train)`\n",
    "\n",
    "and tested similarly as we have tested our other taggers. Train and test it, first on the *news* set then on the big *train*/*test* set. How does it perform compared to your best tagger? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bosIhMcpWXis",
    "outputId": "8dd55c1f-3bff-4428-b177-5929c31e3b8e"
   },
   "outputs": [],
   "source": [
    "hmm_news = benchmark_tagger(nltk.HiddenMarkovModelTagger.train, news_train, news_test)\n",
    "hmm_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrbsBW5AWXix",
    "outputId": "6ff904a6-425b-4730-f57d-b0631464e134"
   },
   "outputs": [],
   "source": [
    "hmm_brown = benchmark_tagger(nltk.HiddenMarkovModelTagger.train, brown_train, brown_test)\n",
    "hmm_brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXBrG5VmWXi2"
   },
   "source": [
    "The hmm model was much faster on training, but slower when predicting. All in all, it was much faster, but the logistic regression model was more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt0lnPHEWXi3"
   },
   "source": [
    "### Part b\n",
    "NLTK also comes with an averaged perceptron tagger which we may train and test. It is currently considered the best tagger included with NLTK. It can be trained as follows:\n",
    "\n",
    "- `per_tagger = nltk.PerceptronTagger(load=False)`\n",
    "- `per_tagger.train(train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noXnmjKuWXi5"
   },
   "source": [
    "It is tested similarly to our other taggers. \n",
    "\n",
    "Train and test it, first on the news set and then on the big train/test set. How does it perform compared to your best tagger? Did you beat it? What about speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "iTv3rwacWXi5",
    "outputId": "84e79fa2-949f-4518-b4bc-05e0a3214189"
   },
   "outputs": [],
   "source": [
    "# I did not have time to run this\n",
    "big_per_tagger = nltk.PerceptronTagger(load=False)\n",
    "big_per_tagger.train(brown_train)\n",
    "big_per_tagger.evaluate(brown_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Jw37n8RHU0zV",
    "outputId": "b2d52f42-a324-498c-fee4-40febb094957"
   },
   "outputs": [],
   "source": [
    "per_tagger = nltk.PerceptronTagger(load=False)\n",
    "per_tagger.train(news_train)\n",
    "per_tagger.evaluate(news_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-8w-uCuWXi9"
   },
   "source": [
    "# Assignment 2 part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "_mTjHy35WXi-",
    "outputId": "7c99bd2d-6d61-4807-b6b6-08bf0a3273b5"
   },
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5xijb69wYLdn",
    "outputId": "bde0ccbe-e772-43c1-fa17-495da038ce4e"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "print(*islice(wv.vocab, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "l5z-6zctWXjL",
    "outputId": "1bfe7adb-9802-4caf-bcf4-323672a399a7"
   },
   "outputs": [],
   "source": [
    "vec_king = wv[\"king\"]\n",
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmI13cHVWXjQ"
   },
   "source": [
    "### Exercise 1 Basics (8 points)\n",
    "a) How many different words are there in the model? With so many words, how come that the\n",
    "‘cameroon’ example fails?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JZTLzDIWXjR"
   },
   "source": [
    "Because cameroon does not exist in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1f2sxkxWXjS"
   },
   "source": [
    "b) Implement a function for calculating the norm (the length) of an (embedding) vector, and a\n",
    "function for calculating the cosine between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggBDRJ76WXjU"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def norm(v):\n",
    "    return sqrt((v**2).sum())\n",
    "\n",
    "def cosine_between(v, u):\n",
    "    return (v @ u)/(norm(v) * norm(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJl63adHWXjZ"
   },
   "source": [
    "c) Calculate the cosine between the vectors for ‘king’ and ‘queen’ and check you get the same as by\n",
    "<model>.similarity(‘king’, ‘queen’)\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sj58QOeyYl4M",
    "outputId": "fabecd37-451d-416e-98d7-58b71d5c6901"
   },
   "outputs": [],
   "source": [
    "vec_queen = wv[\"queen\"]\n",
    "cosine_between(vec_king, vec_queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "gmJpJb30YzWP",
    "outputId": "e8e6e995-d958-471c-e27a-95cc609e6a6c"
   },
   "outputs": [],
   "source": [
    "wv.similarity(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhRMbrptZM2-"
   },
   "source": [
    "yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0rqPPJTWXja"
   },
   "source": [
    "### Exercise 2 Built in functions (5 points)\n",
    "There are several built-in functions that let you inspect semantic properties of the embeddings. The\n",
    "most_similar lets you find the nearest neighbor to one or more words.\n",
    "print(wv.most_similar('car', topn=5))\n",
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAFPqaF9WXjb"
   },
   "source": [
    "a) It is also the tool for testing analogies, e.g. “Norway is to Oslo as Sweden is to …” as\n",
    "\n",
    "```python\n",
    "print(wv.most_similar(positive=['Oslo', 'Sweden'], negative = ['Norway'], topn=5))\n",
    "```\n",
    "Try a few analogy tests like\n",
    "“ king is to man as queen is to …”\n",
    "“ king is to queen as man is to …”\n",
    "“cat is to kitten as dog is to …”\n",
    "Add four more examples of your choice. Report the results of the tests. Are the results as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "Y_dj_k54WXjc",
    "outputId": "a17886a4-6d32-4835-ee54-3b949b0c4dbb"
   },
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['Oslo', 'Sweden'], negative=['Norway'], topn=5))\n",
    "print(wv.most_similar(positive=['Puppy', 'Cat'], negative=['Dog'], topn=5))\n",
    "print(wv.most_similar(positive=['red', 'red'], negative=['blue'], topn=5))\n",
    "print(wv.most_similar(positive=['ecstatic', 'sad'], negative=['happy'], topn=5))\n",
    "print(wv.most_similar(positive=['furious', 'scared'], negative=['angry'], topn=5))\n",
    "print(wv.most_similar(positive=['psychology', 'society'], negative=['brain'], topn=5))\n",
    "print(wv.most_similar(positive=['Satan', 'Luke_Skywalker'], negative=['Jesus'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vmgjpbWWXjh"
   },
   "source": [
    "b) To understand the method a little better, we can try to follow the recipe more directly. Try\n",
    "a = wv['king'] + wv['woman'] - wv['man']\n",
    "and calculate the cosine between a and the vectors for queen, woman, man, king. You may also\n",
    "calculate the\n",
    "wv.similar_by_vector(a)\n",
    "What does this show regarding how the most_similar works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "CR9y92-Xh-o0",
    "outputId": "6423b646-2eae-4239-f4ff-0f8fb7f091ed"
   },
   "outputs": [],
   "source": [
    "a = wv['king'] + wv['woman'] - wv['man']\n",
    "for i in \"queen\", \"woman\", \"man\", \"king\":\n",
    "    print(cosine_between(a, wv[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "E2Ki4BcZje7B",
    "outputId": "21c61125-c4ac-4deb-f865-809d4ed62fe7"
   },
   "outputs": [],
   "source": [
    "wv.similar_by_vector(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEnxOdCGjqzW"
   },
   "source": [
    "The most similar vector is still king and not queen, which counter-argues the analogy hypothesis. man is to king what woman is to king doesnt sound like a good analogy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3BaXL0bWXji"
   },
   "source": [
    "c) Play around with wv.doesnt_match, e.g.\n",
    "print(wv.doesnt_match(['Norway', 'Denmark', 'Finland',\n",
    " 'Sweden', 'Spain', 'Stockholm']))\n",
    "Make at least two more examples where the result match human evaluation and two examples\n",
    "where they do not match. Explain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "UqIj-dKBsNqJ",
    "outputId": "028ca1ff-80d8-4f4d-e515-9ae4c06c762c"
   },
   "outputs": [],
   "source": [
    "groups = [[\"Trumpet\", \"Guitar\", \"Piano\", \"Mayonnaise\", \"Drum\", \"Tuba\"],\n",
    "[\"Captain_America\", \"Spider-Man\", \"Batman\", \"Iron_Man\", \"Superman\", \"Jesus\"],\n",
    "[\"Hilter\", \"Mao\", \"Stalin\", \"Kim_Jon_un\", \"Lenin\", \"The_Joker\"],\n",
    "[\"square\", \"triangle\", \"pentagon\", \"Michelle_Obama\", \"circle\", \"hexagon\"]]\n",
    "\n",
    "for group in groups:\n",
    "  print(\"Which word does not belong of these:\")\n",
    "  print(group)\n",
    "  print(\"Model answered\", wv.doesnt_match(group), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPjFsLl2WXji"
   },
   "source": [
    "### Exercise 3 Training a toy model (5 points)\n",
    "a) Train a word2vec model on the Brown corpus. Follow the recipe from the tutorial, the section\n",
    "Training Your Own Model. You may import the corpus from NLTK by brown.sents(). Beware that this\n",
    "is a toy example. The Brown corpus is too small for training good models. How many times larger is\n",
    "the Google news corpus compared to the Brown corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q55o3BU6uZVu"
   },
   "outputs": [],
   "source": [
    "wv_brown = gensim.models.Word2Vec(sentences=brown.sents()).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Qwkz6Ed9xtu3",
    "outputId": "d56a22fe-f5c5-469a-8ede-0fdb62ce1146"
   },
   "outputs": [],
   "source": [
    "len(wv.vocab)/len(wv_brown.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpBkMUTuy9pw"
   },
   "source": [
    "The vocabulary of the google corpus is almost 200 times larger than the brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT0K2hu0WXjk"
   },
   "source": [
    "b) We will compare the Brown model to the 'word2vec-google-news-300'. Try to find the 10 nearest\n",
    "words first to car and then to queen in the two models. What do the examples reveal about the two\n",
    "training corpora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "cPaC9FzfzMPi",
    "outputId": "df1e4ab0-f4b7-46e3-a785-0d3d193a8ef5"
   },
   "outputs": [],
   "source": [
    "wv.similar_by_word('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "NWAXrHMdzY8K",
    "outputId": "0b936342-0d26-48af-8211-35c573131bc0"
   },
   "outputs": [],
   "source": [
    "wv_brown.similar_by_word(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "gYUC3VjBzVW7",
    "outputId": "cc3784e4-8a70-47ae-a20a-3241b7bc0360"
   },
   "outputs": [],
   "source": [
    "wv.similar_by_word(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "gaxrt_lDzdF_",
    "outputId": "7aebd574-5bdc-482e-f4a7-573790257c5d"
   },
   "outputs": [],
   "source": [
    "wv_brown.similar_by_word(\"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWYJg5-bzkQt"
   },
   "source": [
    "The similarities from the google corpus seem more accurate than those of the brown corpus, which supports the theory that more data usually yields better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaXV3VxwWXjk"
   },
   "source": [
    "c) Inspect the trained Brown model on some of the examples from exercise 2. Does it yield the same\n",
    "results on the analogy tests as the model in exercise 2?\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "a2J6htjHz6qF",
    "outputId": "cd7f0bd4-5f6f-470f-ae6d-4bfc57d4cae0"
   },
   "outputs": [],
   "source": [
    "positives = [['Oslo', 'Sweden'], \n",
    "             ['Puppy', 'Cat'], \n",
    "             ['red', 'red'], \n",
    "             ['ecstatic', 'sad'],\n",
    "             ['furious', 'scared'],\n",
    "             ['psychology', 'society'],\n",
    "             ['Satan', 'Luke_Skywalker']]\n",
    "\n",
    "negatives = [\"Norway\", \"Dog\", \"blue\", \"happy\", \"angry\", \"brain\", \"Jesus\"]\n",
    "\n",
    "for positive, negative in zip(positives, negatives):\n",
    "    try:\n",
    "      print(wv_brown.most_similar(positive=positive, negative=[negative], topn=5))\n",
    "    except KeyError as e:\n",
    "      print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "miF9WIe02-JQ",
    "outputId": "9bea7cab-0a66-47e5-de4c-e5be491263e5"
   },
   "outputs": [],
   "source": [
    "groups = [[\"Trumpet\", \"Guitar\", \"Piano\", \"Mayonnaise\", \"Drum\", \"Tuba\"],\n",
    "[\"Captain_America\", \"Spider-Man\", \"Batman\", \"Iron_Man\", \"Superman\", \"Jesus\"],\n",
    "[\"Hilter\", \"Mao\", \"Stalin\", \"Kim_Jon_un\", \"Lenin\", \"The_Joker\"],\n",
    "[\"square\", \"triangle\", \"pentagon\", \"Michelle_Obama\", \"circle\", \"hexagon\"]]\n",
    "\n",
    "for group in groups:\n",
    "    print(\"Which word does not belong of these:\")\n",
    "    print(group)\n",
    "    try:\n",
    "        print(\"Model answered\", wv_brown.doesnt_match(group), \"\\n\")\n",
    "    except KeyError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-h7c5U73QQ3"
   },
   "source": [
    "In the first example, the results were disasterous, and none of them were the same. In the second one they were ok, as it gave half of the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cVglI7tWXjl"
   },
   "source": [
    "### Exercise 4 Evaluation (5 points)\n",
    "Gensim comes with several methods for evaluation, and also standard datasets for the tests. Testsets\n",
    "could be found by the tha datapath command, e.g.\n",
    "path=datapath('questions-words.txt')\n",
    "One test you may use is to see how well the model perform on the Google analogy test datset. This\n",
    "can be run by\n",
    "<model>.evaluate_word_analogies(path)\n",
    "Report the key numbers, and try to understand what they mean.\n",
    "To compare 'word2vec-google-news-300' to the Brown embeddings is not too interesting. The\n",
    "difference between them is too large. A test like this becomes more interesting if you try to compare\n",
    "'word2vec-google-news-300' to e.g. ‘glove-wiki-gigaword-300’ or you want to inspect the effect of\n",
    "the length of the embeddings by comparing ‘glove-wiki-gigaword-300’ and ‘glove-wiki-gigaword100’.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ervv6mTbWXjD",
    "outputId": "0ade3217-4a09-4360-89e7-fcdd6cf474e5"
   },
   "outputs": [],
   "source": [
    "gwg3 = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "WAf9sU5eWXjG",
    "outputId": "e73926b5-0752-448d-f145-432b04500a14"
   },
   "outputs": [],
   "source": [
    "gwg1 = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vjRJcoP4XHw"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "path = datapath(\"questions-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "h52ksSZq97Of",
    "outputId": "527708a6-8d5b-4bf8-ccbb-da81f00e367f"
   },
   "outputs": [],
   "source": [
    "evaluation = wv.evaluate_word_analogies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XPB6I2t0Mf89",
    "outputId": "714e764b-4c2a-4bb8-a2da-24a866546387"
   },
   "outputs": [],
   "source": [
    "evaluation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "bMz6oQvZMzD6",
    "outputId": "2a3b2937-db4f-40d4-fa9b-57a47a365898"
   },
   "outputs": [],
   "source": [
    "evaluation2 = gwg3.evaluate_word_analogies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LooijplZNA8T",
    "outputId": "673117d9-f8ae-4145-a1c5-0307b33e3fba"
   },
   "outputs": [],
   "source": [
    "evaluation2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "mCKzcb0KMzRk",
    "outputId": "bc8e4a84-34c1-4c2a-95f0-820934f514c8"
   },
   "outputs": [],
   "source": [
    "evaluation3 = gwg1.evaluate_word_analogies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "i-YRMkOwQAdx",
    "outputId": "beed5f3e-d2cc-4db0-90a2-20d4f8cc6072"
   },
   "outputs": [],
   "source": [
    "evaluation3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtAY23FtQD1R"
   },
   "source": [
    "The google news corpus scores higher, while the smaller corpora score lower. The score seems to be proportional with the data size. The score evaluate the trueness of the analogy hypothesis in word-embedding vector spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKB_emCp4CMA"
   },
   "source": [
    "### Exercise 5 Application (12 points)\n",
    "We will try a simple example of applying word embeddings to an NLP task. We consider text\n",
    "classification. We will use the same movie dataset from NLTK as we used in Mandatory assignment\n",
    "1B, with the same split as we used there. Thereby, we may compare the results with the results from\n",
    "Mandatory 1.\n",
    "We will consider a document as a bag of words. The word order and sentence structure will be\n",
    "ignored. Each word can be represented by its embedding. But how should a document be\n",
    "represented? The easiest is to use the “semantic fingerprint”, which means representing the\n",
    "document by the average vector of its words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps9NivoLWXjo"
   },
   "source": [
    "a) Train and test a logistic regression classifier as described. Tune the C parameter (regularization, cf.\n",
    "Mandatory 2.A). Report the results from the tuning in a table. How does this classifier perform\n",
    "compared to your results from Mandatory assignment 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K22le3ltACjl"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "NuKWmZs3KuxU",
    "outputId": "12c639dd-fe3b-402b-9381-b34e2515f5ab"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FtwqVL8AAsA"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_movie_docs = [(movie_reviews.words(fileid), category) for\n",
    "                      category in movie_reviews.categories() for fileid in\n",
    "                      movie_reviews.fileids(category)]\n",
    "except LookupError:\n",
    "    nltk.download('movie_reviews')\n",
    "    raw_movie_docs = [(movie_reviews.words(fileid), category) for\n",
    "                      category in movie_reviews.categories() for fileid in\n",
    "                      movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBG5qG6gAIuU"
   },
   "outputs": [],
   "source": [
    "random.seed(2405)\n",
    "random.shuffle(raw_movie_docs)\n",
    "movie_test = raw_movie_docs[:200]\n",
    "movie_dev  = raw_movie_docs[200:]\n",
    "train_data, dev_test_data = movie_dev[200:], movie_dev[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "N9RRbAinK4Ys",
    "outputId": "c6eb373c-95fb-4943-f86d-f32ec8c79a4d"
   },
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIKceo_EA8Yy"
   },
   "outputs": [],
   "source": [
    "train_texts, train_target = map(list, zip(*train_data))\n",
    "dev_test_texts, dev_test_target = map(list, zip(*dev_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbO-j4kd_w3p"
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h09KbMWYBWm8"
   },
   "outputs": [],
   "source": [
    "train_vectors = [model.infer_vector(doc) for doc in train_texts]\n",
    "dev_test_vectors = [model.infer_vector(doc) for doc in dev_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjMqo9Id8rex"
   },
   "outputs": [],
   "source": [
    "cs = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] \n",
    "fitting_results = np.zeros((6, 2))\n",
    "for i, c in enumerate(cs):\n",
    "    clf = LogisticRegression(C=c)\n",
    "    clf.fit(train_vectors, train_target)\n",
    "    fitting_results[i, 0] = c\n",
    "    fitting_results[i, 1] = clf.score(dev_test_vectors, dev_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "Ajp-xhO-DimL",
    "outputId": "8571eea8-0f52-454a-ad4b-2e0d072f71cc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result1 = pd.DataFrame(fitting_results, columns=[\"C\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRN3FErlDTFf"
   },
   "source": [
    "The results are pretty bad. Count vectorization still seems way better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V06oMLj8WXjp"
   },
   "source": [
    "b) In this task, one could either use the document vectors directly, or normalize the length of each\n",
    "document vector to unit length before classifying. Try both options and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgE0ltzC8swp"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "  norm = np.linalg.norm(x)\n",
    "  return x if norm==0 else x/norm\n",
    "\n",
    "normalized_train = list(map(normalize, train_vectors))\n",
    "normalized_dev_test = list(map(normalize, dev_test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DETc1FIyHx2g"
   },
   "outputs": [],
   "source": [
    "fitting_results2 = np.zeros((6, 2))\n",
    "for i, c in enumerate(cs):\n",
    "    clf = LogisticRegression(C=c)\n",
    "    clf.fit(normalized_train, train_target)\n",
    "    fitting_results2[i, 0] = c\n",
    "    fitting_results2[i, 1] = clf.score(normalized_dev_test, dev_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "4bc2oHnMIZX7",
    "outputId": "6869eb2f-5036-47f7-d63b-46e61244ad0e"
   },
   "outputs": [],
   "source": [
    "result2 = pd.DataFrame(fitting_results2, columns=[\"C\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-eitlrvMYOp"
   },
   "source": [
    "Slightly worse than without normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U78oPS-BWXjq"
   },
   "source": [
    "c) In general, embeddings are used together with neural networks. Scikit-learn provides a simple\n",
    "multi-layered neural network classifier.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "Rerun the classification experiment with this classifier. Try various activation functions and report the\n",
    "results for the various activation functions in a table.\n",
    "Warning! When using neural networks and embeddings for word classification, you would use more\n",
    "elaborate models than the simple bag-of-words model, e.g. convolutional networks or recurrent\n",
    "networks which take word order into consideration.\n",
    "END of Mandatory 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "tRV0t_S6WXjr",
    "outputId": "d32fa45c-5f94-4da9-aaa5-f8edad19a419"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "fitting_results3 = np.zeros((3, 1))\n",
    "activations = (\"relu\", \"logistic\", \"tanh\")\n",
    "\n",
    "for i, activation in enumerate(activations):\n",
    "    nn = MLPClassifier(activation=activation, max_iter=1000)\n",
    "    nn.fit(train_vectors, train_target)\n",
    "    fitting_results3[i, 0] = nn.score(dev_test_vectors, dev_test_target)\n",
    "\n",
    "pd.DataFrame(fitting_results3.T, columns=activations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "in4080_2020_mandatory_1_a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
