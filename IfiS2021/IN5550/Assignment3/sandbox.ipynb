{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from conllu import parse\n",
    "import socket\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    longest_y = max([y.size(0) for X, y in batch])\n",
    "    X = [X for X,y in batch]\n",
    "    y = torch.stack([F.pad(y, (0, longest_y - y.size(0)), value=-1) for X, y in batch])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(tokenizer, ids):\n",
    "    tok_sents = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
    "    mask = []\n",
    "    for sentence in tok_sents:\n",
    "        current = []\n",
    "        for n, token in enumerate(sentence):\n",
    "            if token in tokenizer.all_special_tokens or token.startswith('##'):\n",
    "                continue\n",
    "            else:\n",
    "                current.append(n)\n",
    "        mask.append(current)\n",
    "        \n",
    "    mask = tokenizer.pad({'input_ids': mask}, return_tensors='pt')['input_ids']\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./data/train.conllu\"\n",
    "DEV_PATH = \"./data/dev.conllu\"\n",
    "local_path = 'models/216/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERdata(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label_vocab=None):\n",
    "        self.data = data\n",
    "        self.label_vocab = label_vocab if label_vocab else list(set([token[\"misc\"][\"name\"] for text in data for token in text]))\n",
    "        self.label_vocab.extend(['@UNK'])\n",
    "        self.label_indexer = {i: n for n, i in enumerate(self.label_vocab)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        text = [token[\"form\"] for token in x]\n",
    "        label = [token[\"misc\"][\"name\"] for token in x]\n",
    "    \n",
    "        Y = torch.LongTensor([self.label_indexer[i] if i in self.label_vocab else self.label_indexer['@UNK'] for i in label] )\n",
    "        \n",
    "        return text, Y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERmodel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self._bert = BertModel.from_pretrained('ltgoslo/norbert')\n",
    "        # for param in self._bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "            \n",
    "        self._head = nn.Linear(768, num_labels)\n",
    "        \n",
    "    def forward(self, batch, mask):\n",
    "        b = self._bert(batch)\n",
    "        pooler = b.last_hidden_state[:, mask].diagonal().permute(2,0,1)\n",
    "        return self._head(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERdata(parse(open(TRAIN_PATH, \"r\").read())[0:36*2])\n",
    "val_dataset = NERdata(parse(open(DEV_PATH, \"r\").read())[0:36*2], label_vocab=train_dataset.label_vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=36, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=36, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERmodel(len(train_dataset.label_vocab))\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('ltgoslo/norbert', do_basic_tokenize=False)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:45<00:00, 22.61s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; loss: 0.48966094851493835;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:41<00:00, 20.99s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1; loss: 0.42910173535346985;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:31<00:00, 15.82s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2; loss: 0.5230061411857605;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:31<00:00, 15.89s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3; loss: 0.8582348227500916;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4; loss: 0.8054361939430237;\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for X, y in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X = tokenizer(X, is_split_into_words=True, return_tensors='pt', padding=True)['input_ids']\n",
    "        batch_mask = build_mask(tokenizer, X)\n",
    "        y_pred = model(X, batch_mask).permute(0, 2, 1)\n",
    "        loss = criterion(y_pred, y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for X, y in val_loader:\n",
    "        X = tokenizer(X, is_split_into_words=True, return_tensors='pt', padding=True)['input_ids']\n",
    "        batch_mask = build_mask(tokenizer, X)\n",
    "        y_pred = model(X, batch_mask).permute(0, 2, 1)\n",
    "        #correct += (y_pred.argmax(dim=1) == y.squeeze()).nonzero().size(0)\n",
    "        total += y.size(0)\n",
    "\n",
    "    #print(f\"epoch: {epoch}; loss: {loss.item()}; val. acc = {correct / total}\")\n",
    "    print(f\"epoch: {epoch}; loss: {loss.item()};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
